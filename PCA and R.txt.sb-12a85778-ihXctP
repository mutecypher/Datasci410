[MUSIC PLAYING]

So on various regression transformations,
we haven't asked ourselves what is a good transformation
to the independent features that may create an optimal amount
of independence between them.
Here the idea that we're going to use is to combine the features
into wholly independent and new features by combining them
in various linear fashions.
Imagine we're performing linear progression on multiple features where
a model is as follows.
This is our general multiple linear regression.
We might just proceed forward and assume that our x is
feature is our independent and see how our model performs.
But the question remains, what if we could find new features such
that they are guaranteed to be completely independent of each other
as follows?
Imagine we have the same coefficients out here that we can find,
same definition.
But instead of x1, we have a new function
of all the x f1 of all of the features all the way down to an and function
of all the x features.
If we write it out, it will look like that.
But here the question still remains, how do
we find these magical f functions that assure us
that the new features, these f functions, taken together
are 100% independent of each other?
The method of finding these important components
is called principal component analysis or PCA.
Then when we use these component functions to do regression,
it is called principal component regression or PCR.
It is considered a regularized regression
because, typically, we end up with less features
than we started with because in reality, there's
always going to be some dependence in our data.

You can imagine that, depending on how much linear dependence is in our data
to begin with, the first few completely independent f functions
that we find will explain a majority of the variance in the response
and in the data set.
And the last components will have very little explanatory power.
It is very common to choose a significantly smaller subset
of principal component regressors to do principal component regression.
The benefits principal component regression
is that it assures us that our new features are completely 100%
independent of each other, which overcomes
this multi-collinearity problem in regression.
The downside is that our new features, these f functions,
being the various functions of our original features,
are less interpretable than our original features.
So principal component regression has these steps normally.
We scale the input features in our data.
Then we perform principal component analysis on the scaled
features to find the principal components.
We select a subset of the principal components for the regression.
Usually, we select the number of components
that explain the most variance in the dependent variable.
And then lastly, we take these components
and perform linear regression on them using them as our features.
The reason we cover this regression method after introducing singular value
decomposition is that in order to find these magical functions,
we use the singular value decomposition.
This is used to find these principal components.
For exploring this method, we use generated data
to illustrate how components work on features
that are not fully independent.
To compute principal component regression,
we'll use principal analysis function, PCA
function from the sklearn.decomposition library.
So here we will strictly import the decomposition and a scaling function
for us to use.
The way that we are going to get dependent data on a two
by two x1 and x2 features is we're going to write multiply by a two
by two matrix which will provide, again, a transformation on our random x data.
And if we plot this, you can see that, yes indeed, our two features x and y
are very linearly dependent.
OK so here is the matrix that we're multiplying it by, multiplying by.
Let's look at the first two principal components
because we want to visualize these two principal components in a graph.
So here we compute our principal component analysis
and find and transform our data points onto those principal components.
So here is our principal opponents, our data points
transformed onto the principal axes.
And we are going to plot the output.
So here is our transformed principal component data.

You can see if they are completely unrelated to each other.
We've removed the dependence.
We also look at how much variance in the original features
we are explaining with each principal component.
So now that we fit it, we can look at the explained variance
and the direction of the components, each one of those components.
So the first component explains a majority of the variation.
And the second component does not.
How do we interpret the above variances in components?
For the explained variance, we interpret those values
as the variance of the data set when all the data is
projected onto that f function axis.
So here, the first function or first principle component
explains roughly 1.95 units of the variance.
The second component explains the remaining 0.05 units of variance.
For the components, this tells us the direction of these principal components
that we've found.
In fact, all principal components in this matrix
have the property that they have the magnitude exactly equal to one.
The combination of the two matrices, the explained variance and the component
direction, tells us that we can plot these vectors amongst the original data
to see how they would look.
All right.
So what we're going to use here is the matplotlib lib annotate function.
We're going to plot our original points.
And we're going to plot an arrow in the direction of the principal components.
We're going to scale them up by a factor of two
so we can see them amongst the data.
So here we plotted are two principal components.
From the above plot, we can see that the two vectors,
we plotted are completely independent of each other.
In other words, they're 100% perpendicular.
We can extend this to higher dimensional data and it has many uses.
If we have very high dimensions, we can reduce
the number of dimensions needed for a good regression fit significantly.
So calculating the principal components removes any and all
multi-collinearity issues in the data.
We can project the original data points onto our principal point axes
and perform linear regression.
We will combine everything we've learned into principal component
linear regression with the actual data below.
This time we used the [INAUDIBLE] height data subsetted for females.
Now is a good time to try out our skills to answer a question
about principal component regression.
[MUSIC PLAYING]
