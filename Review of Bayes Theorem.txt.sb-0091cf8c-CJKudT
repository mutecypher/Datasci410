[MUSIC PLAYING]

Hi, and welcome.
So in this lesson, we're going to investigate
more aspects of Bayes models.
And specific, we're going to work toward multi-dimensional or multi-parameter
Bayes models using a hierarchical base structure.
And if you recall, in a previous lesson, we talked about simple Bayes models
where we just tried to estimate one parameter.
So let's just do a little review here to get on the same page
and hopefully, this is all just review to remember what we did earlier.
So let's just write down Bayes' theorem.

And remember, it was the probability of A given B
equals the probability of B given A times the probability of A
over the probability of B. So that's just the basic starting point.
And we rewrote that in a couple of ways to simplify it
because we didn't want to deal with this denominator
here because we're really just trying to find--
we can always re-normalize the posterior distribution.
So let's go ahead and say, we can write this as probability of A given B equals
some constant-- which we'll call K or kappa--
times the probability of B given A probability of A.
And so now we've just said, well, we got some constant here
that we have to include to normalize this to a distribution.
But if we don't even care about that, we can just say,
the probability of A given B is just proportional to the probability
of B given A probability of A.
So that's a simplification that we used, and we had an interpretation for that.
We said that-- we gave these names.
So we said the posterior distribution proportional to what?
To the likelihood times the prior.

And so, you could say, the likelihood is this quantity.
And the prior is this quantity here.
So we had a likelihood and a parameter.
And so how do we apply this?
Well, really applied this that, the probability
of some set of model parameters, theta, given our data
is proportional to the likelihood of the data given those parameters times
our prior distribution on theta.
So again, we just have a likelihood here and a prior.
And so when we want to estimate model parameters,
like the regression coefficients, for example--
slope and intercept or whatever--
given some data, it's the likelihood that those data
could be generated by a model with those coefficients
times our prior distribution for those coefficients.
It's just proportional there.
So that's the summary, which I hope you remember from before,
that we had Bayes' theorem.
But what we're really working with here is just
this business of finding some model coefficients given some data, by using
the likelihood of the data given the coefficients
and a prior of those coefficients.
[MUSIC PLAYING]
